export const metadata = {
  title: 'Brevy Support',
  slug: 'brevy-support',
  time: {
    from: 2022,
    to: 2024,
  },
};

As Brevy pivoted into a AI-powered customer support product for e-commerce companies, so did our codebase.
We cherry-picked modules and libraries from previous iteration, and over 2 years arrived at a complex, 5 microservice backend with a complex data processing pipeline orchestrated by a custom framework.

**Migration to K8s on AWS:** I moved our entire infrastructure to an AWS EKS cluster, managed and maintained through Terraform and Releasehub for CI/CD. We also utilized AWS MQ with the RabbitMQ engine and AWS Aurora running Postgres.

**Gitops:** For resources that were not managed through Releasehub, I introduced a gitops flow with Terraform, deploying resources like helm charts or direct, configurable manifests.

**Infra monitoring:** I configured our monitoring stack to use Prometheus Operator, together with Alert Manager and CloudWatch (through Fluentd) for metrics, displayed in Grafana. On the logs side I introduced Seq, which was the most developer-friendly approach to logs available and all our engineers enjoyed using it daily.

**Generated API Clients from an auto-generated schema:** To avoid wasting dev time on rewriting (often complex) types by hand from the API responses and then writing wrapper hooks/methods to integrate them into frontend data stores, we generated all of those automatically. We already had our OpenAPI schema available, so I chose OpenAPI generator and configured the templates for our usecases. This would run both locally in development, as well as in the CI pipeline.

**Data processing graph:** To make sense and allow for expansion of our very complex, dynamic processing flow, I designed and maintained a processing graph orchestrator with baked in custom rules on top of BullMQ, Redis and Graphology, complete with a UI to display and manage the configuration. It heavily simplified understanding of and expansion of our flow, allowing developers to iterate quickly while creating new or modifying existing processing nodes. At peak volume, it was processing **hundreds of thousands of tickets per month**.

**Vector storage:** With the majority of our data needed in embedded form for model use (tagging, expanding/merging contexts, generating ticket responses, etc.) I introduced Pinecone, a vector storage solution that stored embedded data, synchronized with its' un-embedded form and other metadata. This drastically cut the cost of repeated queries as well as brought reduced latencies to parts that could run on historical data.
This also opened us up to simpler A/B testing of our data pipelines.
